{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notes.* For now, we use [Folktables](https://github.com/socialfoundations/folktables)' ACSIncome prediction task to explore the weighted (causal) partial dependence plot (PDP). I am picturing two possible approaches: \n",
    "\n",
    "(1) For a given year, get all 50 states and like DADT look at train state vs. test state comparisons. Similarly, train on all sates (or a group of states) and focus on drawing insights for some state(s).\n",
    "\n",
    "(2) For a given state (or set of states), train on one year and test on the next year. For instance, we could look at pre- and post-Covid times.\n",
    "\n",
    "Overall, the ML pipeline is to find the best-performing classifiers for the data, where the classifier is a black-box. Through the wPDP we would then show the usefulness of the PDP even under the threat of, say, sample slection or any other sort of distribution shift.\n",
    "\n",
    "The idea is that we train the (black-box) classifier $b()$ on $D_S$ and will deploy it on $D_T$ such that $P_{D_S}(Y, X) \\neq P_{D_T}(Y, X)$. The idea is that we can unse $b()$ for drawing policy-relevant decisions. Since it is a black-box, we are inclined to use PDP to draw the effects of each $X$ on $Y$. We can, in principle, use $b()$ but there is the risk of bias as the weights are relative to $P_{D_S}$: we want to re-weight the PDP to approach the weights under $P_{D_T}$ without having to retrain $b()$.\n",
    "\n",
    "Now, here I want to push for the role of PDP overall with black-box models. I would argue that the application is similar to how, say, economists use *ceteris paribus* for interpreting regression coefficients... I think we can make the case for a linear model (a simple OLS), then one with interaction terms and/or quadratic terms, ... train the model, estimate the betas, and look at the PDP... **can we dran a heuristic here?**\n",
    "\n",
    "If that's the case, we emphasize how important PDP is to $b()$ and also how the re-weighting could help... for this last point, we should then link wPDP with the steps of sample selection where we re-weight the coefficients! Maybe like Imbens's or Heckman's paper, use a simple (structural) model to formulate the problem in a simple way. Such approach will help us to link the wPDP with structural causal models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktables import ACSDataSource, ACSIncome\n",
    "\n",
    "# all US states\n",
    "states = sorted(['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', \n",
    "                 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', \n",
    "                 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', \n",
    "                 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', \n",
    "                 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY'])\n",
    "\n",
    "# state codes from folktables:\n",
    "_STATE_CODES = {'AL': '01', 'AK': '02', 'AZ': '04', 'AR': '05', 'CA': '06',\n",
    "                'CO': '08', 'CT': '09', 'DE': '10', 'FL': '12', 'GA': '13',\n",
    "                'HI': '15', 'ID': '16', 'IL': '17', 'IN': '18', 'IA': '19',\n",
    "                'KS': '20', 'KY': '21', 'LA': '22', 'ME': '23', 'MD': '24',\n",
    "                'MA': '25', 'MI': '26', 'MN': '27', 'MS': '28', 'MO': '29',\n",
    "                'MT': '30', 'NE': '31', 'NV': '32', 'NH': '33', 'NJ': '34',\n",
    "                'NM': '35', 'NY': '36', 'NC': '37', 'ND': '38', 'OH': '39',\n",
    "                'OK': '40', 'OR': '41', 'PA': '42', 'RI': '44', 'SC': '45',\n",
    "                'SD': '46', 'TN': '47', 'TX': '48', 'UT': '49', 'VT': '50',\n",
    "                'VA': '51', 'WA': '53', 'WV': '54', 'WI': '55', 'WY': '56',\n",
    "                'PR': '72'}\n",
    "\n",
    "# data folder\n",
    "data_source = ACSDataSource(survey_year='2017', horizon='1-Year', survey='person', root_dir=os.path.join('..', 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run later\n",
    "for state in bible_belt:\n",
    "    print(state)\n",
    "    \n",
    "    data = data_source.get_data(states=[state], download=True)\n",
    "    features, labels, _ = ACSIncome.df_to_numpy(data)\n",
    "    \n",
    "    df = pd.DataFrame(features, columns=ACSIncome.features)\n",
    "    df['Y'] = labels\n",
    "    df['STATE'] = state\n",
    "    \n",
    "    print(\"{} features for {} individuals\".format(df.shape[1]-1, df.shape[0]))\n",
    "    \n",
    "    df.to_csv(os.path.join('..', 'data', '{}_adult.csv'.format(state.lower())), sep='|', index=False)\n",
    "    \n",
    "    del data, features, labels, df\n",
    "    \n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: for a given state, train an OLS and then use the PDP to see if it serves as a heuristic for Beta\n",
    "# also, consider Loftus's paper: maybe it's the way to get the non-linear relationships!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fair_ml",
   "language": "python",
   "name": "fair_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
